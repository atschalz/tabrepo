# exp_name: "current_test2" # Experiment name 
# # seed: 42 # Random seed to use for experiments
# ignore_cache: true # If true, 
# # run_and_evaluate: true # If true, the experiment is run and evaluated, otherwise only the runs are executed without evaluation



# benchmark:
#     name: "TabArena" # One of: [TabZilla, TabArena, Grinsztajn]
#     subset: ["Food_Delivery_Time"] # If subset is not None, only these datasets are used, otherwise all datasets in the benchmark are used
#     # subset: ["Bank_Customer_Churn", "concrete_compressive_strength", "credit_card_clients_default", "customer_satisfaction_in_airline","Food_Delivery_Time", "houses", "HR_Analytics_Job_Change_of_Data_Scientists", "students_dropout_and_academic_success", "MIC", "qsar-biodeg", "APSFailure", "Bioresponse"]
#     # subset: ["Bank_Customer_Churn", "concrete_compressive_strength", "credit_card_clients_default", "customer_satisfaction_in_airline","Food_Delivery_Time", "houses", "HR_Analytics_Job_Change_of_Data_Scientists", "students_dropout_and_academic_success", "MIC", "qsar-biodeg"]
#     # subset: ["electricity", "nomao", "artificial-characters", "SpeedDating", "airlines", "elevators"]
#     # subset: ["Bike_Sharing_Demand", "nyc-taxi-green-dec-2016", "particulate-matter-ukair-2017", "seattlecrime6", "Allstate_Claims_Severity", "Airlines_DepDelay_1M", "elevators", "Ailerons", "electricity", "eye_movements", "default-of-credit-card-clients", "road-safety", "california", "heloc"]
#     # subset: ["electricity"]
#     outer_folds: "full" # "full" for whole benchmark, or int for user-specified number of outer folds to use in cross-validation
#     repeats: "full" # Number of repeats for outer folds, if outer_folds is not "full"
#     inner_folds: 8 # Number of inner folds for bagging
#     time_limit: 36000 # Time limit for each model in seconds. time_limit=3600 was used in the TabArena 2025 paper
#     # fold_fitting_strategy: "sequential_local"

# Type, name and class can be strings like "LGBModel" or can be objects like LGBModel
# Do not worry about data-types, the framework will handle them

methods:
  - type: "AGModelBagExperiment"
    name: "LightGBM_default"
    model_cls: "LGBModel"
    model_hyperparameters:
      random_state: 0
      n_estimators: 1000
      learning_rate: 0.05
    num_bag_folds: 8
    time_limit: 3600 
    method_kwargs:
      preprocess_data: True
      preprocessor_name: "default_csv"  # Use the default preprocessor for AutoGluon
      fit_kwargs: 
        feature_generator: null

### MY STUFF:
# model:
#     name: "TABM" # 
#     hyperparameters: null  # Model hyperparameters to override the default configurations
#       # n_estimators: 4000
#       # patience: 200

# preprocessing:
#     use_ftd: true
#     input_format: "openml" # One of [openml, csv]
    
    
# hpo: 
#     n_trials: 0 # How many random search trials to run in addition to the default config
#     # ensemble_trials: false # If true, the predictions of all trials are ensembled

# hardware:
    # device: "cuda" # Device to use, currently only "cuda" tested
    # gpus: "0" # Which GPU nodes in cluster to use
    # folds_parallel: 1 # How many CV folds to train in parallel on one node


